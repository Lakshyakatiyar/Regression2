{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f967c26-12ab-4afa-80d7-d0067b25f8a9",
   "metadata": {},
   "source": [
    "1.R-squared (R²):\n",
    "R-squared is a statistical measure that represents the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "\n",
    "Calculation:\n",
    "R-squared is calculated using the formula:\n",
    "\n",
    "𝑅\n",
    "2\n",
    "=\n",
    "1\n",
    "−\n",
    "𝑆\n",
    "𝑆\n",
    "𝑟\n",
    "𝑒\n",
    "𝑠\n",
    "𝑆\n",
    "𝑆\n",
    "𝑡\n",
    "𝑜\n",
    "𝑡\n",
    "R \n",
    "2\n",
    " =1− \n",
    "SS \n",
    "tot\n",
    "​\n",
    " \n",
    "SS \n",
    "res\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "𝑆\n",
    "𝑆\n",
    "𝑟\n",
    "𝑒\n",
    "𝑠\n",
    "SS \n",
    "res\n",
    "​\n",
    "  (Residual Sum of Squares) is the sum of the squares of the residuals (errors).\n",
    "𝑆\n",
    "𝑆\n",
    "𝑡\n",
    "𝑜\n",
    "𝑡\n",
    "SS \n",
    "tot\n",
    "​\n",
    "  (Total Sum of Squares) is the total variance in the dependent variable.\n",
    "Representation:\n",
    "R-squared values range from 0 to 1:\n",
    "\n",
    "0: The model explains none of the variance.\n",
    "1: The model explains all the variance.\n",
    "Interpretation: Higher R-squared indicates a better fit of the model to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb5e1ce-236e-4467-824b-3e12f008cc60",
   "metadata": {},
   "source": [
    "2.Adjusted R-squared:\n",
    "Adjusted R-squared modifies the R-squared value to account for the number of predictors in the model, providing a more accurate measure of model fit when multiple predictors are used.\n",
    "\n",
    "Formula:\n",
    "\n",
    "Adjusted \n",
    "𝑅\n",
    "2\n",
    "=\n",
    "1\n",
    "−\n",
    "(\n",
    "(\n",
    "1\n",
    "−\n",
    "𝑅\n",
    "2\n",
    ")\n",
    "(\n",
    "𝑛\n",
    "−\n",
    "1\n",
    ")\n",
    "𝑛\n",
    "−\n",
    "𝑘\n",
    "−\n",
    "1\n",
    ")\n",
    "Adjusted R \n",
    "2\n",
    " =1−( \n",
    "n−k−1\n",
    "(1−R \n",
    "2\n",
    " )(n−1)\n",
    "​\n",
    " )\n",
    "\n",
    "Where:\n",
    "\n",
    "𝑛\n",
    "n is the number of observations.\n",
    "𝑘\n",
    "k is the number of predictors.\n",
    "Difference from R-squared:\n",
    "\n",
    "Adjusted R-squared penalizes the addition of non-significant predictors, preventing overestimation of model fit.\n",
    "It can decrease if the addition of new predictors does not improve the model sufficientl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5215e4a5-b437-41fd-ac40-cdacf2f269d4",
   "metadata": {},
   "source": [
    "3.Appropriate Use:\n",
    "\n",
    "When comparing models with different numbers of predictors.\n",
    "To avoid overfitting by ensuring that added variables improve the model significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372728be-da02-41a8-a88f-24391defb9fb",
   "metadata": {},
   "source": [
    "4.Root Mean Squared Error (RMSE):\n",
    "RMSE is the square root of the average of the squared differences between the predicted and actual values.\n",
    "\n",
    "RMSE\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "(\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    ")\n",
    "2\n",
    "RMSE= \n",
    "n\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "MSE is the average of the squared differences between the predicted and actual values.\n",
    "\n",
    "MSE\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "(\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    ")\n",
    "2\n",
    "MSE= \n",
    "n\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " \n",
    "\n",
    "Mean Absolute Error (MAE):\n",
    "MAE is the average of the absolute differences between the predicted and actual values.\n",
    "\n",
    "MAE\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "∣\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    "∣\n",
    "MAE= \n",
    "n\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " ∣y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ∣\n",
    "\n",
    "Representation:\n",
    "\n",
    "RMSE and MSE: Penalize larger errors more significantly due to squaring.\n",
    "MAE: Provides a linear score, which is easier to interpret in terms of average prediction error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc7d02a-0c6d-4a10-873b-da7cbce9a6c4",
   "metadata": {},
   "source": [
    "5.RMSE:\n",
    "\n",
    "Advantages: Sensitive to large errors, useful when large errors are particularly undesirable.\n",
    "Disadvantages: Can be overly influenced by outliers.\n",
    "MSE:\n",
    "\n",
    "Advantages: Similar to RMSE, useful for gradient-based optimization algorithms.\n",
    "Disadvantages: Same sensitivity to outliers as RMSE.\n",
    "MAE:\n",
    "\n",
    "Advantages: Robust to outliers, easy to interpret.\n",
    "Disadvantages: Less sensitive to large errors, which can be a drawback in some applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327bc774-c934-4eb6-b3a0-cfbe8f17a98a",
   "metadata": {},
   "source": [
    "6.Lasso Regularization:\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) adds a penalty equivalent to the absolute value of the magnitude of coefficients to the loss function.\n",
    "\n",
    "Formula:\n",
    "\n",
    "Lasso Loss\n",
    "=\n",
    "Loss\n",
    "+\n",
    "𝜆\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑝\n",
    "∣\n",
    "𝛽\n",
    "𝑗\n",
    "∣\n",
    "Lasso Loss=Loss+λ∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " ∣β \n",
    "j\n",
    "​\n",
    " ∣\n",
    "\n",
    "Difference from Ridge Regularization:\n",
    "\n",
    "Ridge Regularization: Adds a penalty equivalent to the square of the magnitude of coefficients (L2 norm).\n",
    "Lasso Regularization: Can shrink some coefficients to zero, effectively performing variable selection.\n",
    "Appropriate Use:\n",
    "\n",
    "When feature selection is needed.\n",
    "When interpreting which variables are significant is important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b53a7f0-5f38-4b37-a332-81116f1156dd",
   "metadata": {},
   "source": [
    "7.Regularized Linear Models:\n",
    "Regularization techniques like Ridge and Lasso add a penalty to the loss function to prevent overfitting by shrinking the magnitude of coefficients.\n",
    "\n",
    "Example:\n",
    "In a dataset with many features, regularized models reduce the complexity by penalizing large coefficients, leading to a more generalizable model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ecfa47-90d3-48e1-ba6f-d0b46ef1198a",
   "metadata": {},
   "source": [
    "8.Limitations:\n",
    "\n",
    "Bias-Variance Trade-off: Regularization introduces bias to reduce variance, which might lead to underfitting.\n",
    "Feature Selection: Lasso can arbitrarily select one among highly correlated features, which might not be ideal.\n",
    "Interpretability: The added penalty can make interpretation less straightforward compared to non-regularized models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ad3ad9-0da8-4cf1-8cfc-17e4247397f1",
   "metadata": {},
   "source": [
    "9.Model A: RMSE = 10\n",
    "Model B: MAE = 8\n",
    "\n",
    "Choice:\n",
    "\n",
    "Without additional context, it’s hard to choose definitively. RMSE penalizes larger errors more, while MAE provides a straightforward average error.\n",
    "Limitation: Different metrics highlight different aspects of model performance. Considering both or additional metrics might provide a more comprehensive evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6291da6e-fe9c-4e6e-b737-c4cbc936930f",
   "metadata": {},
   "source": [
    "10.Model A (Ridge, \n",
    "𝜆\n",
    "=\n",
    "0.1\n",
    "λ=0.1):\n",
    "\n",
    "Ridge regularization shrinks coefficients but does not eliminate them.\n",
    "Suitable when all features are believed to be useful to some extent.\n",
    "Model B (Lasso, \n",
    "𝜆\n",
    "=\n",
    "0.5\n",
    "λ=0.5):\n",
    "\n",
    "Lasso regularization can shrink some coefficients to zero, performing feature selection.\n",
    "Suitable when feature selection is desired.\n",
    "Choice:\n",
    "\n",
    "Trade-offs: Lasso's ability to zero out coefficients is beneficial for sparse models but might discard useful correlated features. Ridge keeps all features but can lead to complex models.\n",
    "Appropriate Use: Depends on the importance of feature selection versus retaining all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ea2c2-2a06-4a62-8ead-adf8f466972e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
